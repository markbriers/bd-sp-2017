{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 Instructions and Exercises (Big Data in Statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notes contain instructions and questions for the labs portion of the Big Data in Statistics module. Within this document, command-line steps are presented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -put data /user/mark/repository/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All commands will be in a separate grey \"cell\" (as above).\n",
    "<br><br>\n",
    "Exercises will be listed as a bulleted item and italicized. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><i>Create a new directory in your HDFS home directory called sample. Upload data.csv into the sample directory on HDFS.</i></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow real-world development practices, you will be using configuration control software git, and internet based repositories on <a href=\"http://github.com\">github.com</a>. Instructions will be provided on how to use these tools during the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will be expected to achieve the following:\n",
    "<ol>\n",
    "<li>Connect to the Big Data infrastructure and download data from github; </li>\n",
    "<li>Move data into and out of HDFS;</li>\n",
    "<li>Execute a WordCount MapReduce job;</li>\n",
    "<li>Create and execute a LineCount MapReduce job.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the cluster, follow these instructions:\n",
    "<ol>\n",
    "<li>Start Xming (type xming into the search bar on the start menu).</li>\n",
    "<li>Start Putty (type putty into the search bar on the start menu).</li>\n",
    "<li>In the hostname textbox, type bazooka.ma.ic.ac.uk, ensuring that Connection type is set to ssh.</li>\n",
    "<li>Navigate to Connection –> SSH -> X11, and ensure “Enable X11 forwarding” is selected.</li>\n",
    "<li>Click Open.</li>\n",
    "<li>On first connection to bazooka, you may be presented with a dialogue box asking a “host key” related question. Please click “Yes” in response to this question.</li>\n",
    "<li>You will be asked for your username and password.</li>\n",
    "<li>Usernames are userN (with N replaced by your allocated number) and initial password.</li>\n",
    "<li>On first login, you will be asked to change your password. In current password, enter the initial password, then press enter. You will then be asked to type your new password, and then to confirm your new password. You will be notified if your password has been successfully changed.</li>\n",
    "<li>Close putty. Now start from instruction 2 again. On this occasion you will enter your new password and will be able to continue with the exercises below.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now connected to the Hadoop environment, where you will perform all labs-based exercises and your coursework for this module.\n",
    "<br><br>\n",
    "Hadoop is available from the command line via the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing this command will display a help message on the console. Please take a look at the printed help, as you may wish to consult such help during the exercises.\n",
    "<br><br>\n",
    "You are now going to download all of the supporting material for this lab. This will be done using git. For those of you that would like to learn more about git, please consult <a href=\"https://git-scm.com/book/en/v2/Getting-Started-Git-Basics\">Git basics</a>. However, the instructions within this document (and subsequent documents) should be sufficient for your needs.\n",
    "<br><br>\n",
    "Type the following command to download the course material:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/markbriers/bd-sp-2017.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a short delay (whilst the material is downloaded), you should be able to change your working directory to M5MS19 via the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd bd-sp-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second command will list the contents of the folder M5MS19. There are four directories.\n",
    "<br><br>\n",
    "You have now connected to the Hadoop cluster, have taken a first look at the available Hadoop commands, and have cloned the git repository that we will use throughout the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now change your working directory to the data subfolder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As listed, there are four files in this directory: <i>sample.txt</i>, <i>textdata.txt</i>, <i>heathrowdata.txt</i> and <i>wickairportdata.txt</i>.\n",
    "You are now going to interact with the Hadoop Distributed Filesystem (HDFS). HDFS help can be obtained by executing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now put the <i>sample.txt</i> file from your local machine into your home folder on HDFS, renaming the file to be <i>sampledata.txt</i> on HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -put sample.txt sampledata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second command lists all files and folders in your HDFS home directory. You should see the </i>sampledata.txt</i> file listed as existing in HDFS.<br>\n",
    "The reverse operation, to get data from HDFS onto your local disk, is executed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -get sampledata.txt ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The period at the end of the command implies that you want to place the file <i>sampledata.txt</i> into your current working directory, and you do not wish to rename the file. <b>NB: Downloading data from HDFS can cause problems; data in HDFS is often very large, so please be careful when using the get command.</b>\n",
    "<br><br>\n",
    "You will now make a new directory inside your HDFS home directory using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -mkdir temperatureData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second command lists all files and folders in your HDFS home directory. You should see the <i>temperatureData</i> directory listed as existing in HDFS.\n",
    "<br><br>\n",
    "You will now upload <i>heathrowdata.txt</i> and <i>wickairportdata.txt</i> into the <i>temperatureData</i> directory in HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -put heathrowdata.txt temperatureData/heathrowdata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -put wickairportdata.txt temperatureData/wickairportdata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -ls temperatureData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the new files in HDFS now listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Create a new folder in HDFS called textData. Put the file textData.txt into this new folder on HDFS. Confirm that the upload has been successful by listing the contents of the textData directory.</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now be executing a Map Reduce job that performs word count. First, change your working directory to w1e3 (the folder naming convention is <b>w</b>eek <b>1</b> <b>e</b>xercise <b>3</b>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ../w1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that there are two files in this folder; a mapper and a reducer. You will need to update the properties of these files, so that they have execute permissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chmod +x *.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command modifies the permissions on all python files (with <i>.py</i> extension) so that they can be executed via the bash shell.<br>\n",
    "Examine these files using your favourite Linux text editor (instructions for vimcan be found <a href=\"http://www.engadget.com/2012/07/10/vim-how-to/\">here</a>; instructions for nano can be found <a href=\"https://www.howtogeek.com/howto/42980/the-beginners-guide-to-nano-the-linux-command-line-text-editor/\">here</a>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nano mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nano reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that you understand the contents of each Python file.\n",
    "\n",
    "You are now going to execute a Hadoop job across the files in the <i>textData</i> directory, using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_STR/hadoop-streaming-2.7.0-mapr-1808.jar \\\n",
    "-libjars $HADOOP_STR/hadoop-streaming-2.7.0-mapr-1808.jar \\\n",
    "-input textData/* \\\n",
    "-output w1e3-output \\\n",
    "-mapper \"python mapper.py\" \\\n",
    "-file /home/USERNAME/bd-sp-2017/w1e3/mapper.py \\\n",
    "-reducer \"python reducer.py\" \\\n",
    "-file /home/USERNAME/bd-sp-2017/w1e3/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that <b>USERNAME</b> will need to be replaced with your compute cluster username, <i>userN</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After execution, results will be output to the w1e3-output folder on HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>List the contents of the w1e3-output folder.<br>\n",
    "How many part files are produced?<br>\n",
    "What does this number indicate about the number of reducer processes that were executed?<br>\n",
    "What should we have done to improve the output? [HINT: Look at the keyspace and punctuation.]</i></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to view the contents of the results is to <i>cat</i> the files into the Linux less paging utility function, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -cat w1e3-output/part* | less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your space bar to page through the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change your working directory to w1e4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ../w1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chmod +x *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Using the stub mapper.py and reducer.py functions in the w1e4 working directory, write and execute a Map Reduce program that:<br>\n",
    "<ol>\n",
    "<li>computes the length of each line during the Map phase, outputting (lineLength, 1) as the (key, value) pair\n",
    "<li>counts the number of occurrences of lineLength during the reduce phase, outputting (lineLength, count) as (key, value) pairs. View the results using the cat and less utility functions. What do you notice about the results?\n",
    "</ol>\n",
    "</i></li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
