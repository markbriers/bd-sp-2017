{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Instructions and Exercises (Big Data in Statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notes contain instructions and questions for the labs portion of the Big Data in Statistics module. Within this document, command-line steps are presented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -put data /user/mark/repository/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All commands will be in a separate grey \"cell\" (as above).\n",
    "<br><br>\n",
    "Exercises will be listed as a bulleted item and italicized. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><i>Create a new directory in your HDFS home directory called sample. Upload data.csv into the sample directory on HDFS.</i></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow real-world development practices, you will be using configuration control software git, and internet based repositories on <a href=\"http://github.com\">github.com</a>. Instructions will be provided on how to use these tools during the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will be expected to achieve the following:\n",
    "<ol>\n",
    "<li>Create a Spark Resilient Distributed Dataset (RDD)\n",
    "<li>Transform an RDD\n",
    "<li>Perform an action on an RDD\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now connect to the Spark REPL, using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a short delay, you will be presented with a new prompt that looks like follows (NB: it is safe to ignore the error messages when Spark loads):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid ambiguity, all Spark commands listed within this document will follow this <i>scala</i> prompt.<br>\n",
    "As part of the Spark startup process, a Spark context (sc) is created, which is the main interface to the requested Spark environment. To view the available functions, type the following and then immediately press the Tab key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> sc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create an RDD by loading the Heathrow airport temperature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val heathrowFile = sc.textFile(\"hdfs:///user/USERNAME/temperatureData/heathrowdata.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that <b>USERNAME</b> will need to be replaced with your compute cluster username, <i>userN</i>.\n",
    "<br><br>\n",
    "As displayed in the console, the value heathrowFile is of type org.apache.spark.rdd.RDD[String], that is, the function textFile has created an RDD of type String (as one would perhaps expect when reading a file of text). Due to Scala’s lazy evaluation model, the file has not been read; the file has not even been checked to see if it exists! This only happens when an action is performed. You will now bring back 5 lines of data to the console and print each line, to force Spark to undertake an <i>action</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val fiveLines = heathrowFile.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> fiveLines.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line creates a value fiveLines of the RDD by taking the first 5 elements (lines in the case of a textFile). fiveLines is of type Array[String] as the memory is now being held and managed in the local console, rather than via Spark’s RDD. The second line calls the function println on each element of fiveLines, which subsequently displays the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Create a value called wickairportFile, that is an RDD[String], based on the wickairportdata.txt file in HDFS. Display the first five lines of this data file.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create a function that is able to determine whether a line of data contains the header information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> def isHeader(line: String): Boolean = {\n",
    "line.contains(\"yyyy\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is created by using the keyword def, followed by the function name, input arguments, and output type. The function body is specified in between the braces. This function simply checks to see whether the string yyyy is present in the line of data that is passed into the function. The string yyyy is only present in the header line.\n",
    "<br><br>\n",
    "You will now create a Case class corresponding to the structure of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> case class TemperatureRecord(year: Long, month: Int, tmax: Float, tmin: Float, rain: Float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class will act as a data structure to allow us to parse and manipulate the data in a convenient manner. You can create an RDD of TemperatureRecord objects by creating a parse function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> def parse(line: String) = {\n",
    "val pieces = line.split('\\t')\n",
    "val year = pieces(0).toLong\n",
    "val mm = pieces(1).toInt\n",
    "val tmax = pieces(2).toFloat\n",
    "val tmin = pieces(3).toFloat\n",
    "val rain = pieces(4).toFloat\n",
    "TemperatureRecord(year, mm, tmax, tmin, rain)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and by creating the following transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val heathrowData = heathrowFile.filter(x => !isHeader(x)).map(parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, due to Spark’s lazy evaluation model, this command will not be executed until we perform an action on the data. The value heathrowData is of type org.apache.spark.rdd.RDD[TemperatureRecord]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Perform the same transformation on the value wickairportFile, to create a value wickairportData, which is of type org.apache.spark.rdd.RDD[TemperatureRecord]. NB: You will need to create an additional filter transformation to filter the lines of data in the wickairportFile that contain missing values (i.e. the records that contain “---“ as default data files). [It is recommended that you test this set of transformations using the take command.]</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often the case that you need to sort the data in accordance with one of the fields in the data. You will now sort the data by month rather than year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala > val heathrowMonth = heathrowData.sortBy(_.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Sort the wickairportData by maximum temperature, creating a value called wickairportTemp. <b>NB: As often occurs in data analysis, you will need to clean the data first, by filtering any erroneous data points that contain \"---\" as the temperature values.</b><i>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now convert all temperature values from degrees Celsius into degrees Fahrenheit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scala> val heathrowMonthFah = heathrowMonth.map( x => {\n",
    "val tmax = x.tmax * 9/5 + 32\n",
    "val tmin = x.tmin * 9/5 + 32\n",
    "TemperatureRecord(x.year, x.month, tmax, tmin, x.rain)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Create an RDD[Float] containing the monthly rainfall in centimeters for Wick airport (the rainfall data is represented in mm in the data), sorted by tmax.</i>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
