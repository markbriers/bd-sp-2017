{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 Instructions and Exercises (Big Data in Statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notes contain instructions and questions for the labs portion of the Big Data in Statistics module. Within this document, command-line steps are presented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs –put data /user/mark/repository/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All commands will be in a separate grey \"cell\" (as above).\n",
    "<br><br>\n",
    "Exercises will be listed as a bulleted item and italicized. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><i>Create a new directory in your HDFS home directory called sample. Upload data.csv into the sample directory on HDFS.</i></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow real-world development practices, you will be using configuration control software git, and internet based repositories on <a href=\"http://github.com\">github.com</a>. Instructions will be provided on how to use these tools during the exercises.\n",
    "<br><br>\n",
    "<b>For these exercises, before using putty to connect to the compute cluster, please remember to execute Xming first.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will be expected to achieve the following:\n",
    "<ol>\n",
    "<li>Create and execute a Map Reduce job with a Combiner\n",
    "<li>Create and execute a Map Reduce job to search for specific data\n",
    "<li>Create and execute a Map Reduce job to join two datasets\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now going to update your copy of the the supporting material for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ~/bd-sp-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice six additional folders corresponding to the five exercises (and a coursework folder). You should now change your working directory to the w2e1 subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd w2e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chmod +x *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third command will list the contents of this folder. As you will see, the folder contains three python files; <i>mapper.py</i>, <i>reducer.py</i> and <i>combiner.py</i>.\n",
    "You will now execute a distributed word count map reduce job that uses a combiner to reduce the amount of network traffic prior to the shuffle and sort phase of the Map Reduce process. This is executed via the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop jar $HADOOP_STR/hadoop-0.20.2-dev-streaming.jar \\\n",
    "-libjars $HADOOP_STR/hadoop-0.20.2-dev-streaming.jar \\\n",
    "-input textData/* \\\n",
    "-output w2e1-output \\\n",
    "-mapper \"python mapper.py\" \\\n",
    "-file /home/USERNAME/bd-sp-2017/w2e1/mapper.py \\\n",
    "-reducer \"python reducer.py\" \\\n",
    "-file /home/USERNAME/bd-sp-2017/w2e1/reducer.py \\\n",
    "-combiner “combiner.py” \\\n",
    "-file /home/USERNAME/bd-sp-2017/w2e1/combiner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that <b>USERNAME</b> will need to be replaced with your compute cluster username, <i>userN</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now change your working directory to w2e2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ../w2e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chmod +x *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls –la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Using the pre-populated mapper.py and reducer.py functions, write a combiner function (using combiner.py) that improves the reducer efficiency of the Map Reduce program. This map reduce process should be executed across the files in the textData directory in HDFS.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now change your working directory to w2e3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ../w2e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chmod +x *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls –la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temperatureData folder on HDFS contains two text files that are populated with monthly meteorological data from Heathrow airport (London) and Wick airport (Scotland) between 1948 to 2015. The columns within each data file correspond to the following headers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i>Year ~ Month ~ MaxTemp ~ MinTemp ~ Rainfall</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical SQL operation performed within a relational database is to join two (or more) tables, using a specified join key (this is the same as the merge function in R). In the case of the temperature data, one could join the data on Year-Month, listing the Heathrow airport values and then the Wick airport values on the same line. That is, a line from the joined data (by Year-Month) would for formatted as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i>Year ~ Month ~ HMaxTemp ~ HMinTemp ~ HRainfall ~ WMaxTemp ~ WMinTemp ~ WRainfall</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the <i>H</i> prefix corresponds to data from Heathrow airport, and the <i>W</i> prefix corresponds to data from Wick airport, for the same Year and Month. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Write a Map Reduce program that joins the two temperature datasets (temperatureData/heathrowdata.txt and temperatureData/wickairportdata.txt in HDFS) using Year-Month as the join key. The output should be the consistent with the header line defined directly above.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now change your working directory to w2e4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ../w2e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chmod +x *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This folder contains two python files (<i>mapper.py</i> and <i>reducer.py</i>) that do not contain any python code. You will be required to write the python code for the following exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Write a Map Reduce program that outputs data for <b>September only</b> using the joined results from Exercise 3. Get your results from HDFS and place them in the local filesystem. Using R or matplotlib in Python (R is available by typing R at the command line prompt and/or Python is available by typing python at the command line prompt) plot two time-series of maximum temperature data, sorted by year, for each location (Heathrow airport and Wick airport). What do you notice about the results?</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now change your working directory to w2e5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd ../w2e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chmod +x *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This folder contains two python files (<i>mapper.py</i> and <i>reducer.py</i>) that do not contain any python code. You are required to write the python code for the following exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><i>Write a Map Reduce program to compute the parameters of an ordinary least squares model. Apply the programme to estimate \"maximum temperature\" data, where y = Heathrow airport data and x = Wick airport data.</i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
